{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd25acd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "import pandasql as ps\n",
    "import sqlite3\n",
    "import random\n",
    "import numpy as np\n",
    "from shapely.geometry import Point\n",
    "import pandas as pd\n",
    "from geopandas import GeoDataFrame\n",
    "import geopandas as gpd\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import graphviz\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from datetime import datetime, timedelta, date\n",
    "import gc\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, accuracy_score, classification_report, confusion_matrix, precision_score, recall_score, ConfusionMatrixDisplay\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from scipy.stats import randint\n",
    "import scipy.stats as st\n",
    "\n",
    "from sklearn.tree import export_graphviz\n",
    "from IPython.display import Image\n",
    "import graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867f45dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to read the file and store it as a Pandas Dataframe.\n",
    "def read_dataframe (path , filename):\n",
    "    df = pd.read_csv(path + \"/\" + filename)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c51cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function that drops specific columns.\n",
    "def drop_columns(df , columns):\n",
    "    df.drop( columns , axis=1 , inplace=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e7332f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function that drops columns with all null values.\n",
    "def drop_all_null_columns(df):\n",
    "    df.dropna(how = 'all' , axis=1 , inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce786de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function that drops rows with null values in a particular column.\n",
    "def drop_rows(df , column_checker):\n",
    "    indices_to_drop = df[ df[ column_checker ].isnull() ].index\n",
    "    df.drop(indices_to_drop , inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65bed69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function that fills NaN or NA column with median of that column.\n",
    "def fill_column_with_median(df , column):\n",
    "    df[column] = df[column].fillna(df[column].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178fbf3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function that fills NaN or NA column with some value.\n",
    "def fill_column_with_values(df , column , value):\n",
    "    df[column] = df[column].fillna(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30ca11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function that converts a string object to datetime-format.\n",
    "def convert_column_to_datetime(df , column):\n",
    "    df[column] = pd.to_datetime(df[column])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c091f509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function that converts extracts year from a datetime object.\n",
    "def extract_year_to_new_column(df , column , new_column):\n",
    "    df[new_column] = df[column].dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a2482e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function that converts extracts month from a datetime object and is stored as a string-object.\n",
    "def extract_month_to_new_column(df , column , new_column):\n",
    "    df[new_column] = df[column].dt.month\n",
    "    df[new_column] = df[new_column].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223bc6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function that makes new column with some default value. For example, a column with all 0's.\n",
    "def make_new_column(df , new_column , default_values):\n",
    "    df[new_column] = pd.Series([default_values for x in range(len(df.index))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3520ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function that sets some entries of a column to a particular value based on some condition.\n",
    "def set_conditional_values_column(df , column , condition , value):\n",
    "    df.loc[ condition , column ] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27de7237",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function that gets a dummy column for a categorical column, and attaches them to the dataframe.\n",
    "def get_dummies_for_categorical_columns(df , column):\n",
    "    df_dummy = pd.get_dummies(df[column], prefix = column)\n",
    "    drop_columns(df , column)\n",
    "    return pd.concat( [df , df_dummy] , axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6200453",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function that makes 0/1 labels from a dataframe's column.\n",
    "def make_labels(df , column , value_to_zero , value_to_one):\n",
    "    labels = pd.DataFrame(df[column])\n",
    "    labels[column][labels[column] == value_to_zero] = 0\n",
    "    labels[column][labels[column] == value_to_one] = 1\n",
    "    labels = labels.apply(pd.to_numeric)\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2eff4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function that adds makes and adds dummy columns for multiple categorical columns to a dataframe. \n",
    "def add_dummy_columns_to_df(df , list_of_cols):\n",
    "    df_temp = df\n",
    "    for col in list_of_cols:\n",
    "        df_temp = get_dummies_for_categorical_columns(df_temp , col)\n",
    "    return df_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35929a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function that makes a dictionary from two one-to-one columns, and sorts them in an ascending fasion\n",
    "def make_and_sort_dictionary_by_values(lk , lv):\n",
    "    res = {}\n",
    "    for k in lk:\n",
    "        for v in lv:\n",
    "            res[k] = v\n",
    "            lv.remove(v)\n",
    "            break\n",
    "\n",
    "    keys = list(res.keys())\n",
    "    values = list(res.values())\n",
    "    n = len(values)\n",
    "    abs_values = [abs(ele) for ele in values]\n",
    "    sorted_value_index = np.argsort(abs_values)#[::-1][:n]\n",
    "    sorted_dict = {keys[i]: values[i] for i in sorted_value_index}\n",
    "    return sorted_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd0add4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function that normalizes all numeric columns of a dataframe. \n",
    "def normalize_all_numeric_columns( df ):\n",
    "    list_of_numeric_columns = list( df.select_dtypes(include='number').columns )\n",
    "    for column in list_of_numeric_columns:\n",
    "        df[column] = ( df[column] - df[column].mean() ) / df[column].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8020b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function that runs a balanced logisitc regression num_of_sims times using a test-size picked randomly from a list. It also stores a certain number of important coefficients by their weighted size\n",
    "def logistic_regression_simulation( num_of_sims , list_test_size , num_of_coefs_stored ):\n",
    "    data = []\n",
    "    for i in range(num_of_sims):\n",
    "        ts = random.choice(list_test_size)\n",
    "        rs = random.randint(1, 10000)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(df , labels ,  test_size = ts , random_state = rs )\n",
    "        logmodel = LogisticRegression(class_weight='balanced')\n",
    "        logmodel.fit(X_train, y_train)\n",
    "        predictions_log = logmodel.predict(X_test)\n",
    "        importance = logmodel.coef_.flatten()\n",
    "        cm = confusion_matrix(y_test, predictions_log)\n",
    "        accuracy = accuracy_score(y_test, predictions_log)\n",
    "        lr_probs = logmodel.predict_proba(X_test)\n",
    "        lr_probs = lr_probs[:, 1]\n",
    "        lr_auc = roc_auc_score(y_test, lr_probs)\n",
    "        importance = logmodel.coef_.flatten()\n",
    "        sorted_dict = make_and_sort_dictionary_by_values( list(df.columns) , list(importance) )\n",
    "        for k in sorted_dict:\n",
    "            sorted_dict[k] = sorted_dict[k] * sum(df[k]) / len(df[k])\n",
    "        sorted_dict = make_and_sort_dictionary_by_values( list(sorted_dict.keys()) , list(sorted_dict.values()) )\n",
    "        lk = list(sorted_dict.keys())[::-1][:num_of_coefs_stored] \n",
    "        lv = list(sorted_dict.values())[::-1][:num_of_coefs_stored]\n",
    "        res = {}\n",
    "        for k in lk:\n",
    "            for v in lv:\n",
    "                res[k] = v\n",
    "                lv.remove(v)\n",
    "                break\n",
    "        data.append([ts , rs , cm , accuracy , lr_auc, res])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0bfd21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function that spits out various performance metrics and their confidence interval\n",
    "def performance_metrics(data , confidence_int):\n",
    "    s1 = []\n",
    "    s2 = []\n",
    "    s3 = []\n",
    "    s4 = []\n",
    "    s5 = []\n",
    "    s6 = []\n",
    "    s7 = []\n",
    "    s8 = []\n",
    "    for pt in data:\n",
    "        s1.append(pt[3])\n",
    "        s2.append(pt[4])\n",
    "        cm = pt[2]\n",
    "        s3.append( cm[0,0] / (cm[0,0] + cm[0,1]) )\n",
    "        s4.append( cm[1,1] / (cm[1,0] + cm[1,1]) )\n",
    "        s5.append( cm[1,0] / (cm[1,0] + cm[1,1]) )\n",
    "        s6.append( cm[0,1] / (cm[0,0] + cm[0,1]) )\n",
    "        s7.append( cm[0,0] / (cm[0,0] + cm[1,0]) )\n",
    "        s8.append( cm[1,1] / (cm[0,1] + cm[1,1]) )\n",
    "\n",
    "    a = s1\n",
    "    print( \"Accuracy is \" , sum(a) / len(a) * 100 )\n",
    "    print(\"Accuracy's \" + str(confidence_int*100) + \"% confidence interval is\" , st.t.interval(alpha = confidence_int, df=len(a)-1, loc=np.mean(a), scale=st.sem(a)))\n",
    "    print(\"\\n\")\n",
    "    del s1\n",
    "\n",
    "    a = s2\n",
    "    print( \"ROC-AUC is \" , sum(a) / len(a) )\n",
    "    print(\"ROC-AUC's \" + str(confidence_int*100) + \"% confidence interval is\" , st.t.interval(alpha = confidence_int, df=len(a)-1, loc=np.mean(a), scale=st.sem(a)))\n",
    "    print(\"\\n\")\n",
    "    del s2\n",
    "\n",
    "    a = s3\n",
    "    print( \"TPR is \" , sum(a) / len(a) * 100 )\n",
    "    print(\"TPR's \" + str(confidence_int*100) + \"% confidence interval is\" , st.t.interval(alpha = confidence_int, df=len(a)-1, loc=np.mean(a), scale=st.sem(a)))\n",
    "    print(\"\\n\")\n",
    "    del s3\n",
    "\n",
    "    a = s4\n",
    "    print( \"TNR is \" , sum(a) / len(a) * 100 )\n",
    "    print(\"TNR's \" + str(confidence_int*100) + \"% confidence interval is\" , st.t.interval(alpha = confidence_int, df=len(a)-1, loc=np.mean(a), scale=st.sem(a)))\n",
    "    print(\"\\n\")\n",
    "    del s4\n",
    "\n",
    "    a = s5\n",
    "    print( \"Type I error is \" , sum(a) / len(a) * 100 )\n",
    "    print(\"Type I's \" + str(confidence_int*100) + \"% confidence interval is\" , st.t.interval(alpha = confidence_int, df=len(a)-1, loc=np.mean(a), scale=st.sem(a)))\n",
    "    print(\"\\n\")\n",
    "    del s5\n",
    "\n",
    "    a = s6\n",
    "    print( \"Type II error is \" , sum(a) / len(a) * 100 )\n",
    "    print(\"Type II's \" + str(confidence_int*100) + \"% confidence interval is\" , st.t.interval(alpha = confidence_int, df=len(a)-1, loc=np.mean(a), scale=st.sem(a)))\n",
    "    print(\"\\n\")\n",
    "    del s6\n",
    "\n",
    "    a = s7\n",
    "    print( \"Precision is \" , sum(a) / len(a) * 100 )\n",
    "    print(\"Precision's \" + str(confidence_int*100) + \"% confidence interval is\" , st.t.interval(alpha = confidence_int, df=len(a)-1, loc=np.mean(a), scale=st.sem(a)))\n",
    "    print(\"\\n\")\n",
    "    del s7\n",
    "\n",
    "    a = s8\n",
    "    print( \"Recall is \" , sum(a) / len(a) * 100 )\n",
    "    print(\"Recall's \" + str(confidence_int*100) + \"% confidence interval is\" , st.t.interval(alpha = confidence_int, df=len(a)-1, loc=np.mean(a), scale=st.sem(a)))\n",
    "    print(\"\\n\")\n",
    "    del a,s8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a91aeb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function that spits out the most important coefficients that appeared in all simulations and their confidence interval\n",
    "\n",
    "def important_coefs(data , confidence_int):\n",
    "    set1 = set(data[0][5].keys())\n",
    "    for i in range(num_of_sims):\n",
    "        set1 = set1 & set(data[i][5].keys())\n",
    "    list_of_coefs = list(set1)\n",
    "    list_of_coefs\n",
    "\n",
    "    for coef in list_of_coefs:\n",
    "        d1 = []\n",
    "        for i in range(num_of_sims):\n",
    "            d1.append(data[i][5][coef])\n",
    "        print( \"Average of \" + coef + \" is \" , sum(d1) / len(d1) )\n",
    "        print( str(confidence_int*100) + \"% confidence interval of \" + coef + \" is \" , st.t.interval(alpha = confidence_int , df=len(d1)-1 , loc=np.mean(d1) , scale=st.sem(d1)) )\n",
    "        print(\"\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f266136c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = read_dataframe(\"Downloads\" , \"loan_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2b10e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa77b387",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_all_null_columns(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290c651e",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_rows(df , 'term')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7fc9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fill_column_with_values(df , 'emp_length' , '0 years')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71d1378",
   "metadata": {},
   "outputs": [],
   "source": [
    "fill_column_with_median(df , 'mths_since_last_delinq')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d33f655",
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_column_to_datetime(df , 'earliest_cr_line')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d1110b",
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_year_to_new_column(df , 'earliest_cr_line' , 'year_earliest_cr_line')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27eab066",
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_month_to_new_column(df , 'earliest_cr_line' , 'month_earliest_cr_line')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94f4a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_columns(df , ['int_rate2' , 'int_rate3' , 'earliest_cr_line' , 'id' , 'loan_amnt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9ee990",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95b0aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_new_column(df , 'loan_good_or_bad' , 'good')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f51f75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(['loan_status']).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b484f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_conditional_values_column(df , \n",
    "                              'loan_good_or_bad' , \n",
    "                              (df['loan_status'] == 'Charged Off') | \n",
    "                              (df['loan_status'] == 'Default') , \n",
    "                              'bad')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c10d33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_columns(df , 'loan_status')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6992605",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = add_dummy_columns_to_df( df , [ 'term' , \n",
    "                                     'emp_length' , \n",
    "                                     'home_ownership' , \n",
    "                                     'purpose' , \n",
    "                                     'addr_state' , \n",
    "                                     'month_earliest_cr_line' ] )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e22a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = make_labels(df , 'loan_good_or_bad' , 'good' , 'bad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d3f623",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_columns(df , 'loan_good_or_bad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19aa2d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize_all_numeric_columns( df )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845ef522",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_test_size = [0.1 , 0.15, 0.2, 0.25, 0.3, 0.35, 0.4]\n",
    "\n",
    "num_of_sims = 1000\n",
    "\n",
    "coefs_stored = 10\n",
    "\n",
    "data = logistic_regression_simulation(num_of_sims , list_of_test_size , coefs_stored )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602c1b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence_int = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce0b022",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "performance_metrics(data, confidence_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a24e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "important_coefs(data , confidence_int)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
